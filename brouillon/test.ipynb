{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "Assistant: Oui, vous parlez bien à un modèle d'intelligence artificielle développé par OpenAI. Comment puis-je vous aider aujourd'hui ?\n",
      "Assistant: Je suis basé sur le modèle GPT-4 d'OpenAI. En quoi puis-je vous aider aujourd'hui ?\n",
      "Assistant: Pour créer un serveur basique avec FastAPI, tu auras besoin d'installer FastAPI et un serveur ASGI comme Uvicorn. Voici comment tu peux le faire :\n",
      "\n",
      "1. Installe FastAPI et Uvicorn via pip (tu peux le faire dans ton terminal ou dans un environnement virtuel) :\n",
      "\n",
      "   ```bash\n",
      "   pip install fastapi uvicorn\n",
      "   ```\n",
      "\n",
      "2. Crée un fichier Python, par exemple `main.py`, et ajoute le code suivant :\n",
      "\n",
      "   ```python\n",
      "   from fastapi import FastAPI\n",
      "\n",
      "   app = FastAPI()\n",
      "\n",
      "   @app.get(\"/\")\n",
      "   async def read_root():\n",
      "       return {\"Hello\": \"World\"}\n",
      "\n",
      "   @app.get(\"/items/{item_id}\")\n",
      "   async def read_item(item_id: int, q: str = None):\n",
      "       return {\"item_id\": item_id, \"q\": q}\n",
      "   ```\n",
      "\n",
      "   Voici une explication rapide du code ci-dessus :\n",
      "\n",
      "   - Le serveur FastAPI est créé avec `FastAPI()`.\n",
      "   - Deux routes sont définies : une route de base (`/`) qui renvoie un message simple et une route dynamique (`/items/{item_id}`) qui accepte un paramètre `item_id` et un paramètre de requête optionnel `q`.\n",
      "\n",
      "3. Lance le serveur avec Uvicorn :\n",
      "\n",
      "   ```bash\n",
      "   uvicorn main:app --reload\n",
      "   ```\n",
      "\n",
      "   - `main:app` indique à Uvicorn de chercher l'application `app` dans le fichier `main.py`.\n",
      "   - L'option `--reload` active le rechargement automatique, ce qui est utile pour le développement car il recharge le serveur chaque fois que tu modifies le code.\n",
      "\n",
      "4. Accède à l'application en ouvrant un navigateur et en visitant `http://127.0.0.1:8000`. Tu devrais voir une réponse JSON `{\"Hello\": \"World\"}`.\n",
      "\n",
      "5. Pour explorer la documentation interactive de l'API générée automatiquement par FastAPI, tu peux aller sur `http://127.0.0.1:8000/docs`. FastAPI utilise Swagger UI pour fournir cette interface.\n",
      "\n",
      "C'est tout pour mettre en place un serveur FastAPI basique ! Tu peux maintenant ajouter plus de routes et de fonctionnalités selon tes besoins.\n",
      "Assistant: Hello! How can I assist you today?\n",
      "Assistant: Hello! How can I assist you today?\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass\n",
    "\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
